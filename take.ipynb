{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Caption Generation\n",
    "\n",
    "### Introduction\n",
    "Image Caption generation is a challenging problem in AI that connects computer vision and NLP where a textual description must be generated for a given photograph. In General Sense for a given image as input, our model describes the exact description of an Image. It requires both image understanding from the domain of computer vision which Convolution Neural Network and a language model from the field of Natural language processing.\n",
    "It is important to assume and test multiple ways to frame a given predictive modeling problem and there are indeed many ways to frame the problem of generating captions for photographs. \n",
    "\n",
    "So Basically what our model does is when we pass an image to our CNN and RNN combined architecture then it will generate the natural description of the image using NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pickle\n",
    "import os\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# this file is located in pytorch tutorial/image\n",
    "# captioning which we pull from git remember\n",
    "import sys\n",
    "sys.path.append('C:/Deep Learning/Projects/Code')  # Add once at the start\n",
    "\n",
    "# Now import normally\n",
    "from build_vocab import Vocabulary\n",
    "from model import EncoderCNN, DecoderRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Function to Load and Resize the Image\n",
    "def load_image(image_path, transform=None):\n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize([224, 224], Image.LANCZOS)\n",
    "    if transform is not None:\n",
    "        image = transform(image).unsqueeze(0)\n",
    "    return image\n",
    "\n",
    "# Model paths\n",
    "ENCODER_PATH = 'C:/Deep Learning/Projects/Code/Image Caption Generation/Model/encoder-5-3000.pkl'\n",
    "DECODER_PATH = 'C:/Deep Learning/Projects/Code/Image Caption Generation/Model/decoder-5-3000.pkl'\n",
    "VOCAB_PATH = 'C:/Deep Learning/Projects/Code/Image Caption Generation/vocab.pkl'\n",
    "\n",
    "# Constants\n",
    "EMBED_SIZE = 256\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(decoder, features, vocab, beam_width=5, max_len=20):\n",
    "    \"\"\"\n",
    "    Implements beam search for better caption generation.\n",
    "    \n",
    "    Args:\n",
    "        decoder: DecoderRNN model\n",
    "        features: Encoded image features from CNN\n",
    "        vocab: Vocabulary wrapper\n",
    "        beam_width: Number of beams to keep track of\n",
    "        max_len: Maximum length of the caption\n",
    "        \n",
    "    Returns:\n",
    "        best_caption: The caption with highest probability\n",
    "    \"\"\"\n",
    "    # Move tensors to the same device as the model\n",
    "    device = next(decoder.parameters()).device\n",
    "    \n",
    "    k = beam_width\n",
    "    \n",
    "    # Initialize the first word as <start> for all k beams\n",
    "    start_token = vocab.word2idx['<start>']\n",
    "    end_token = vocab.word2idx['<end>']\n",
    "    \n",
    "    # Initialize sequence scores\n",
    "    top_k_scores = torch.zeros(k, 1).to(device)\n",
    "    # Generate k starting sequences with just the start token\n",
    "    seqs = torch.full((k, 1), start_token, dtype=torch.long).to(device)\n",
    "    \n",
    "    # Lists to store completed sequences and their scores\n",
    "    complete_seqs = []\n",
    "    complete_seqs_scores = []\n",
    "    \n",
    "    # Initialize hidden and cell states\n",
    "    states = None\n",
    "    \n",
    "    # Start beam search\n",
    "    step = 1\n",
    "    while True:\n",
    "        if step == 1:\n",
    "            # For the first step, we only have one sequence (the start token)\n",
    "            # Forward pass through decoder\n",
    "            embedded = decoder.embed(seqs[0].unsqueeze(0))\n",
    "            if states is None:\n",
    "                outputs, states = decoder.lstm(embedded, None)\n",
    "            else:\n",
    "                outputs, states = decoder.lstm(embedded, states)\n",
    "            outputs = decoder.fc(outputs.squeeze(1))\n",
    "            \n",
    "            # Get top k words\n",
    "            scores = F.log_softmax(outputs, dim=1)\n",
    "            top_k_scores, top_k_words = scores[0].topk(k, dim=0)\n",
    "            \n",
    "            # Convert to tensors\n",
    "            top_k_scores = top_k_scores.unsqueeze(1)\n",
    "            \n",
    "            # Create k sequences, each with the start token and one top word\n",
    "            seqs = torch.full((k, 2), start_token, dtype=torch.long).to(device)\n",
    "            seqs[:, 1] = top_k_words\n",
    "            \n",
    "            # Ensure states are properly expanded for k beams\n",
    "            if isinstance(states, tuple):  # LSTM states\n",
    "                # Each state is (h, c) where h and c are (num_layers, batch_size, hidden_size)\n",
    "                h, c = states\n",
    "                states = (h.expand(-1, k, -1).contiguous(), \n",
    "                            c.expand(-1, k, -1).contiguous())\n",
    "            else:  # GRU states\n",
    "                states = states.expand(-1, k, -1).contiguous()\n",
    "        else:\n",
    "            # Subsequent steps\n",
    "            curr_input = seqs[:, -1]\n",
    "            embedded = decoder.embed(curr_input)\n",
    "            if isinstance(states, tuple):  # LSTM\n",
    "                h, c = states\n",
    "                batch_size = h.size(1)\n",
    "            else:  # GRU\n",
    "                batch_size = states.size(1)\n",
    "            \n",
    "            embedded = embedded.view(1, batch_size, -1)\n",
    "            outputs, states = decoder.lstm(embedded, states)\n",
    "            outputs = decoder.fc(outputs.squeeze(1))\n",
    "            \n",
    "            # Get scores for next words\n",
    "            scores = F.log_softmax(outputs, dim=1)\n",
    "            \n",
    "            # Add the log probabilities to current scores\n",
    "            scores = top_k_scores.expand_as(scores) + scores\n",
    "            \n",
    "            # For the first batch, we consider all k possibilities\n",
    "            # For subsequent batches, we consider k^2 possibilities\n",
    "            if step == 2:\n",
    "                top_k_scores, top_k_words = scores[0].topk(k, dim=0)\n",
    "            else:\n",
    "                # Flatten all scores\n",
    "                top_k_scores, top_k_words = scores.view(-1).topk(k, dim=0)\n",
    "                \n",
    "                # Get the beam indices and word indices\n",
    "                prev_word_inds = top_k_words // scores.size(1)  # beam index\n",
    "                next_word_inds = top_k_words % scores.size(1)   # word index\n",
    "                \n",
    "                # Update sequences\n",
    "                new_seqs = []\n",
    "                for i, w in enumerate(prev_word_inds):\n",
    "                    new_seqs.append(torch.cat([seqs[w], next_word_inds[i].unsqueeze(0)], dim=0))\n",
    "                \n",
    "                seqs = torch.stack(new_seqs)\n",
    "                \n",
    "                # Update states based on beam indices\n",
    "                if isinstance(states, tuple):  # LSTM\n",
    "                    h, c = states\n",
    "                    h = h.view(-1, batch_size, h.size(2))\n",
    "                    c = c.view(-1, batch_size, c.size(2))\n",
    "                    states = (h[:, prev_word_inds], c[:, prev_word_inds])\n",
    "                else:  # GRU\n",
    "                    states = states.view(-1, batch_size, states.size(2))\n",
    "                    states = states[:, prev_word_inds]\n",
    "        \n",
    "        # Check for completed sequences\n",
    "        is_end = seqs[:, -1] == end_token\n",
    "        if step > 1:\n",
    "            # Add complete sequences to our results\n",
    "            for i in range(len(is_end)):\n",
    "                if is_end[i]:\n",
    "                    complete_seqs.append(seqs[i].tolist())\n",
    "                    complete_seqs_scores.append(top_k_scores[i])\n",
    "            \n",
    "            # If all k beams are finished, exit\n",
    "            k -= sum(is_end).item()\n",
    "            if k == 0:\n",
    "                break\n",
    "                \n",
    "            # Remove completed beams\n",
    "            seqs = seqs[~is_end]\n",
    "            top_k_scores = top_k_scores[~is_end]\n",
    "            if isinstance(states, tuple):\n",
    "                h, c = states\n",
    "                states = (h[:, ~is_end], c[:, ~is_end])\n",
    "            else:\n",
    "                states = states[:, ~is_end]\n",
    "        \n",
    "        # Stop if we've reached the maximum length\n",
    "        if step >= max_len:\n",
    "            break\n",
    "        \n",
    "        # Increment step\n",
    "        step += 1\n",
    "    \n",
    "    # If we have completed sequences, return the best one\n",
    "    if complete_seqs:\n",
    "        best_seq_idx = complete_seqs_scores.index(max(complete_seqs_scores))\n",
    "        best_seq = complete_seqs[best_seq_idx]\n",
    "    else:\n",
    "        # If no sequence is complete, return the current best one\n",
    "        best_seq_idx = top_k_scores.argmax().item()\n",
    "        best_seq = seqs[best_seq_idx].tolist()\n",
    "    \n",
    "    # Convert sequence of indices to words\n",
    "    words = []\n",
    "    for idx in best_seq:\n",
    "        # Skip start and pad tokens in the output\n",
    "        if idx != vocab.word2idx['<start>'] and idx != vocab.word2idx['<pad>']:\n",
    "            if idx == vocab.word2idx['<end>']:\n",
    "                break\n",
    "            words.append(vocab.idx2word[idx])\n",
    "    \n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified PretrainedResNet function to use beam search\n",
    "def PretrainedResNetBeamSearch(image_path, encoder_path=ENCODER_PATH,\n",
    "                    decoder_path=DECODER_PATH,\n",
    "                    vocab_path=VOCAB_PATH,\n",
    "                    embed_size=EMBED_SIZE,\n",
    "                    hidden_size=HIDDEN_SIZE,\n",
    "                    num_layers=NUM_LAYERS,\n",
    "                    beam_width=5):\n",
    "    \n",
    "    # Import the torch.nn.functional module if not imported\n",
    "    import torch.nn.functional as F\n",
    "    \n",
    "    # Image preprocessing\n",
    "    transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                                    (0.229, 0.224, 0.225))])\n",
    "    \n",
    "    # Load vocabulary wrapper\n",
    "    with open(vocab_path, 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "    \n",
    "    # Build models\n",
    "    # eval mode (batchnorm uses moving mean/variance)\n",
    "    encoder = EncoderCNN(embed_size).eval()\n",
    "    decoder = DecoderRNN(embed_size, hidden_size, len(vocab), num_layers)\n",
    "    \n",
    "    # Pass device as string directly\n",
    "    if device == 'cuda':\n",
    "        encoder = encoder.cuda()\n",
    "        decoder = decoder.cuda()\n",
    "    \n",
    "    # Load the trained model parameters\n",
    "    encoder.load_state_dict(torch.load(encoder_path, map_location=device))\n",
    "    decoder.load_state_dict(torch.load(decoder_path, map_location=device))\n",
    "    \n",
    "    # Prepare an image\n",
    "    image = load_image(image_path, transform)\n",
    "    # Move image to device\n",
    "    if device == 'cuda':\n",
    "        image_tensor = image.cuda()\n",
    "    else:\n",
    "        image_tensor = image\n",
    "    \n",
    "    # Generate a caption from the image\n",
    "    feature = encoder(image_tensor)\n",
    "    \n",
    "    # Use beam search instead of greedy decoding\n",
    "    sentence = beam_search(decoder, feature, vocab, beam_width=beam_width)\n",
    "    \n",
    "    # Apply title case to the sentence\n",
    "    sentence = sentence.title()\n",
    "    \n",
    "    # Return the generated caption and the image\n",
    "    image = Image.open(image_path)\n",
    "    return sentence, image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\k\\anaconda3\\envs\\dL-pT\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\k\\anaconda3\\envs\\dL-pT\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DecoderRNN' object has no attribute 'fc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Deep Learning/coco/images/train2014/COCO_train2014_000000001424.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Use the beam search version with beam width of 5\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m predicted_label, image \u001b[38;5;241m=\u001b[39m \u001b[43mPretrainedResNetBeamSearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_width\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(image)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(predicted_label)\n",
      "Cell \u001b[1;32mIn[5], line 49\u001b[0m, in \u001b[0;36mPretrainedResNetBeamSearch\u001b[1;34m(image_path, encoder_path, decoder_path, vocab_path, embed_size, hidden_size, num_layers, beam_width)\u001b[0m\n\u001b[0;32m     46\u001b[0m feature \u001b[38;5;241m=\u001b[39m encoder(image_tensor)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Use beam search instead of greedy decoding\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m sentence \u001b[38;5;241m=\u001b[39m \u001b[43mbeam_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_width\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeam_width\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Apply title case to the sentence\u001b[39;00m\n\u001b[0;32m     52\u001b[0m sentence \u001b[38;5;241m=\u001b[39m sentence\u001b[38;5;241m.\u001b[39mtitle()\n",
      "Cell \u001b[1;32mIn[4], line 47\u001b[0m, in \u001b[0;36mbeam_search\u001b[1;34m(decoder, features, vocab, beam_width, max_len)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     46\u001b[0m     outputs, states \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mlstm(embedded, states)\n\u001b[1;32m---> 47\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc\u001b[49m(outputs\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Get top k words\u001b[39;00m\n\u001b[0;32m     50\u001b[0m scores \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlog_softmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\k\\anaconda3\\envs\\dL-pT\\lib\\site-packages\\torch\\nn\\modules\\module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DecoderRNN' object has no attribute 'fc'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x1200 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "image_path = 'C:/Deep Learning/coco/images/train2014/COCO_train2014_000000001424.jpg'\n",
    "# Use the beam search version with beam width of 5\n",
    "predicted_label, image = PretrainedResNetBeamSearch(image_path=image_path, beam_width=5)\n",
    "plt.imshow(image)\n",
    "print(predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dL-pT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
